

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Description &mdash; Opteeq 1.0 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="opteeq" href="modules.html" />
    <link rel="prev" title="&lt;no title&gt;" href="index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> Opteeq
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Description</a></li>
<li class="toctree-l1"><a class="reference internal" href="#table-of-contents">Table of Contents</a></li>
<li class="toctree-l1"><a class="reference internal" href="#installation-configuration">Installation / Configuration</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#get-the-project-from-the-repository">get the project from the repository:</a></li>
<li class="toctree-l2"><a class="reference internal" href="#install-library-requirements">Install library requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="#config-the-aws-cli">Config the aws cli:</a></li>
<li class="toctree-l2"><a class="reference internal" href="#config-the-project">Config the project</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#step-1-data-preparation-with-aws-pipeline">Step 1: Data preparation with AWS pipeline</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#aws-structure">AWS structure</a></li>
<li class="toctree-l2"><a class="reference internal" href="#image-uploading">1.1 Image uploading</a></li>
<li class="toctree-l2"><a class="reference internal" href="#image-standardization">1.2 Image standardization</a></li>
<li class="toctree-l2"><a class="reference internal" href="#image-automatic-pre-annotation">1.3 Image automatic pre-annotation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#example-of-results">1.3.1 Example of results</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#manual-labelling">1.4 Manual labelling</a></li>
<li class="toctree-l2"><a class="reference internal" href="#pipeline-cost-estimation">1.5 Pipeline cost estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#usage">Usage</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#upload-image">Upload image</a></li>
<li class="toctree-l3"><a class="reference internal" href="#generate-json-for-via">Generate JSON for via</a></li>
<li class="toctree-l3"><a class="reference internal" href="#download-image-and-json">Download image and JSON</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#step-2-ai-modelling-with-yolov4-and-cutie">Step 2: AI modelling with YOLOv4 and CUTIE</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#yolov4">2.1 YOLOv4</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#data-preprocessing">2.1.1 Data preprocessing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#installation">2.1.2 Installation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#install-nvidia-docker">2.1.2.1 Install Nvidia docker</a></li>
<li class="toctree-l4"><a class="reference internal" href="#build-the-docker-image">2.1.2.2 Build the docker image</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#model-training">2.1.3 Model training</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model-evaluation">2.1.4 Model evaluation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#detection">2.1.4 Detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="#detection-without-darknet-framework">2.1.4 Detection without Darknet framework</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#cutie">2.2 CUTIE</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">2.2.1 Data preprocessing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id2">2.1.2 Model training</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#models-benchmarking">2.3 Models Benchmarking</a></li>
<li class="toctree-l2"><a class="reference internal" href="#conclusion">2.4 Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#step-3-web-application-deployment">Step 3: Web application deployment</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#deployement">3.1 Deployement</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#lambda">3.1.1 lambda</a></li>
<li class="toctree-l3"><a class="reference internal" href="#static-file">3.1.2 Static file</a></li>
<li class="toctree-l3"><a class="reference internal" href="#demo">3.1.3 Demo</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#license">License</a></li>
<li class="toctree-l1"><a class="reference internal" href="#team">Team</a></li>
<li class="toctree-l1"><a class="reference internal" href="#sphinx-documentation">Sphinx Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html">opteeq</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Opteeq</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Description</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/info.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="tex2jax_ignore mathjax_ignore section" id="description">
<h1>Description<a class="headerlink" href="#description" title="Permalink to this headline">¶</a></h1>
<p>Opteeq is a student project that uses computer vision and AI modeling for receipt digitalisation.</p>
<p><strong>Objective</strong>: To build a receipt digitalization app that extracts key information (
Place, Date and Total amount of expense) from paper receipts.</p>
<p><strong>Methodology</strong></p>
<ol class="simple">
<li><p>Data preparation with AWS pipeline (finished)</p></li>
<li><p>AI modelling with YOLOv4 and CUTIE (ongoing)</p></li>
<li><p>Web application interface deployment (not started)</p></li>
</ol>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="table-of-contents">
<h1>Table of Contents<a class="headerlink" href="#table-of-contents" title="Permalink to this headline">¶</a></h1>
<p><a class="reference external" href="#installation--configuration">Installation / Configuration</a></p>
<p><a class="reference external" href="#step-1-data-preparation-with-aws-pipeline">Step 1: Data preparation with AWS pipeline</a></p>
<p><a class="reference external" href="#step-2-ai-modelling-with-yolov4-and-cutie">Step 2: AI modelling with YOLOv4 and CUTIE</a></p>
<p><a class="reference external" href="#step-3-web-application-deployment-tbd">Step 3: Web application deployment (TBD)</a></p>
<p><a class="reference external" href="#license">License</a></p>
<p><a class="reference external" href="#team">Team</a></p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="installation-configuration">
<h1>Installation / Configuration<a class="headerlink" href="#installation-configuration" title="Permalink to this headline">¶</a></h1>
<div class="section" id="get-the-project-from-the-repository">
<h2>get the project from the repository:<a class="headerlink" href="#get-the-project-from-the-repository" title="Permalink to this headline">¶</a></h2>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git clone https://github.com/assansanogo/Opteeq.git
</pre></div>
</div>
</div>
<div class="section" id="install-library-requirements">
<h2>Install library requirements<a class="headerlink" href="#install-library-requirements" title="Permalink to this headline">¶</a></h2>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip3 install -r requirements.txt
</pre></div>
</div>
</div>
<div class="section" id="config-the-aws-cli">
<h2>Config the aws cli:<a class="headerlink" href="#config-the-aws-cli" title="Permalink to this headline">¶</a></h2>
<p>To add another profile <code class="docutils literal notranslate"><span class="pre">aws</span> <span class="pre">configure</span> <span class="pre">--profile</span> <span class="pre">profilName</span></code> to list available profiles <code class="docutils literal notranslate"><span class="pre">aws</span> <span class="pre">configure</span> <span class="pre">list-profiles</span></code>.</p>
</div>
<div class="section" id="config-the-project">
<h2>Config the project<a class="headerlink" href="#config-the-project" title="Permalink to this headline">¶</a></h2>
<p>Before using edit conf.json. you need to edit:</p>
<ol class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">user</span></code>: your username.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bucket_raw</span></code>: bucket where raw image are upload</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bucket_standardized</span></code>: bucket where standardized image are upload</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bucket_initial_annotation</span></code>: bucket where initial json annotation are uploaded</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dynamoDB</span></code>: region and table name</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">profile</span></code>: choose the profile to use if many CLI profile are set up on your computer otherwise let <code class="docutils literal notranslate"><span class="pre">default</span></code>.</p></li>
</ol>
<p>To add another profile <code class="docutils literal notranslate"><span class="pre">aws</span> <span class="pre">configure</span> <span class="pre">--profile</span> <span class="pre">profilName</span></code> to list available profiles <code class="docutils literal notranslate"><span class="pre">aws</span> <span class="pre">configure</span> <span class="pre">list-profiles</span></code></p>
<p>If the part that you will use doesn’t need one of these parameters you can ignore it.</p>
<p>When you create the dynamoDB add a global secondary index on anotName and named annotator-index.</p>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="step-1-data-preparation-with-aws-pipeline">
<h1>Step 1: Data preparation with AWS pipeline<a class="headerlink" href="#step-1-data-preparation-with-aws-pipeline" title="Permalink to this headline">¶</a></h1>
<p>A database of ~1,300 receipt pictures has been collected. In order to help with the tedious labelling work, a pipeline
has been developped in AWS to pre-annotate the pictures.<br />
Each receipt added to the database is standardized and scanned using AWS Rekognition for text recognition. The pipeline
then reformats the annotations and computes JSON files containing batches of 20 images. These JSON files can be imported
into VGG Image Anotator for the final manual part of the labelling.</p>
<p>All the data is stored in Amazon Simple Storage Service (S3) cloud storage buckets, and key information is extracted and
saved to a noSQL database using Amazon DynamoDB.</p>
<div class="section" id="aws-structure">
<h2>AWS structure<a class="headerlink" href="#aws-structure" title="Permalink to this headline">¶</a></h2>
<p><img alt="image" src="_images/aws_Diagram.png" /></p>
</div>
<div class="section" id="image-uploading">
<h2>1.1 Image uploading<a class="headerlink" href="#image-uploading" title="Permalink to this headline">¶</a></h2>
<p>Python script is used to rename and upload all raw images (.jpeg, .png, .tiff) of receipts into an S3
bucket <code class="docutils literal notranslate"><span class="pre">bucket_raw</span></code> and rename them with the name of the uploader and an index.</p>
</div>
<div class="section" id="image-standardization">
<h2>1.2 Image standardization<a class="headerlink" href="#image-standardization" title="Permalink to this headline">¶</a></h2>
<p>Raw images are transformed using an AWS Lambda function into standardized images.The Lambda function runs on an S3
trigger based on a put event (the file upload). Image standardization includes a minimum image size check, image
resizing, text orientation detection and rotation.</p>
<p><em>The steps include:</em></p>
<ol class="simple">
<li><p>Raw images are read from <code class="docutils literal notranslate"><span class="pre">bucket_standardized</span></code> using Boto3 and OpenCV.</p></li>
<li><p>The text orientation of the image is checked</p></li>
<li><p>The image is rotated if needed</p></li>
<li><p>The image size is compared with the Google Cloud Vision API recommended image size for document text recognition (
1067x768)</p></li>
<li><p>The image is resized either using height or width (1067 or 768) while keeping the same aspect ratio to avoid
distorting the image</p></li>
<li><p>The processed image is written locally to ‘tmp’ directory with a unique filename - ‘IMG-{uuid4}-{unix_timestamp}.jpg’
using OpenCV.</p></li>
<li><p>Additional datapoints are added to the database including a unique ID, image name, raw name, and uploader name.</p></li>
</ol>
<p>Standardized images are then pushed (uploaded) into <code class="docutils literal notranslate"><span class="pre">bucket_standardized</span></code> using Boto3.</p>
</div>
<div class="section" id="image-automatic-pre-annotation">
<h2>1.3 Image automatic pre-annotation<a class="headerlink" href="#image-automatic-pre-annotation" title="Permalink to this headline">¶</a></h2>
<p>AWS Rekognition API is called to pre-annotate the pictures with boxes around the text. Annotations from Rekognition are
then converted by batch to a json file that can be imported
in <a class="reference external" href="https://www.robots.ox.ac.uk/~vgg/software/via/">VGG Image Annotator</a>. The goal of this step is essentially to reduce
and ease the manual labelling of the pictures that will be done in the next step.</p>
<div class="section" id="example-of-results">
<h3>1.3.1 Example of results<a class="headerlink" href="#example-of-results" title="Permalink to this headline">¶</a></h3>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><img alt="via0" src="_images/via0.png" /></p></th>
<th class="text-align:center head"><p><img alt="via" src="_images/via.png" /></p></th>
<th class="text-align:right head"><p><img alt="via2" src="_images/via2.png" /></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p></p></td>
<td class="text-align:center"><p></p></td>
<td class="text-align:right"><p></p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="manual-labelling">
<h2>1.4 Manual labelling<a class="headerlink" href="#manual-labelling" title="Permalink to this headline">¶</a></h2>
<p>JSON files are imported in VGG Image Annotator. The only remaining part is to assign the Date, Place and Total Amount
classes to the relevant boxes. This is perfomed manually by team members. Final annotations are exported as csv files
and uploaded into <strong>AWS Bucket 3</strong>. A Lambda function runs on an S3 trigger based on a put event to update the database
for all the pictures found in the annotation file.</p>
</div>
<div class="section" id="pipeline-cost-estimation">
<h2>1.5 Pipeline cost estimation<a class="headerlink" href="#pipeline-cost-estimation" title="Permalink to this headline">¶</a></h2>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Product</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Cost (USD/month)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>AWS S3 Buckets</p></td>
<td><p>Cloud storage</p></td>
<td><p>1,38</p></td>
</tr>
<tr class="row-odd"><td><p>Amazon Simple Queue Service (SQS)</p></td>
<td><p>web service for storing messages in transit between computers</p></td>
<td><p>0,01</p></td>
</tr>
<tr class="row-even"><td><p>Amazon Lambda Function</p></td>
<td><p>serverless compute service that runs code in response to events</p></td>
<td><p>0,44</p></td>
</tr>
<tr class="row-odd"><td><p>Amazon Elastic Compute Cloud (Amazon EC2)</p></td>
<td><p>allows users to rent virtual computers to run their own computer applications</p></td>
<td><p>1,01</p></td>
</tr>
<tr class="row-even"><td><p>Rekognition</p></td>
<td><p>API for text detection image processing</p></td>
<td><p>10,00</p></td>
</tr>
<tr class="row-odd"><td><p>Cloudwatch</p></td>
<td><p>monitoring and management service for AWS</p></td>
<td><p>2,58</p></td>
</tr>
<tr class="row-even"><td><p>DynamoDB</p></td>
<td><p>NoSQL database service</p></td>
<td><p>0,04</p></td>
</tr>
<tr class="row-odd"><td><p>TOTAL</p></td>
<td><p>total cost for first month without free tier</p></td>
<td><p>$15,46</p></td>
</tr>
</tbody>
</table>
<p><strong>Notes:</strong></p>
<ul class="simple">
<li><p>With Free-tier, total costs should be below $10 for 10,000 images.</p></li>
<li><p>After labelling, the files will be moved to Glacier as a zip.</p></li>
</ul>
</div>
<div class="section" id="usage">
<h2>Usage<a class="headerlink" href="#usage" title="Permalink to this headline">¶</a></h2>
<p>You can use all this script with the command behind or use the notebook pipeline if you prefer.</p>
<p>All this script is in a package it is recommended to lunch them with <code class="docutils literal notranslate"><span class="pre">-m</span></code>. If you don’t use <code class="docutils literal notranslate"><span class="pre">-m</span></code> you can have import
error or path error.</p>
<div class="section" id="upload-image">
<h3>Upload image<a class="headerlink" href="#upload-image" title="Permalink to this headline">¶</a></h3>
<ol>
<li><p>put the image in the image folder</p></li>
<li><p>execute</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="sb">`</span>python3 -m pipeline_aws.rename_upload<span class="sb">`</span>
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="generate-json-for-via">
<h3>Generate JSON for via<a class="headerlink" href="#generate-json-for-via" title="Permalink to this headline">¶</a></h3>
<ol>
<li><p>Start the Ec2 with this user data (compatible Debian and Ubuntu):</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
sudo apt update
sudo apt install python3-pip -y
<span class="nb">cd</span> home/<span class="nv">$USER</span>
git clone https://github.com/assansanogo/Opteeq.git
<span class="nb">cd</span> Opteeq
pip3 install -r requirements.txt
</pre></div>
</div>
</li>
<li><p>execute:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3 -m pipeline_aws.ec2
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="download-image-and-json">
<h3>Download image and JSON<a class="headerlink" href="#download-image-and-json" title="Permalink to this headline">¶</a></h3>
<ol>
<li><p>Execute:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python -m pipeline_aws.download
</pre></div>
</div>
</li>
<li><p>Go to <a class="reference external" href="https://www.robots.ox.ac.uk/~vgg/software/via/via.html">VGG Image Annotator 2</a>, <strong>open a VIA project</strong> and
choose output.json. (If the image file can’t be found, download the HTML file and change the default path in the
settings)</p></li>
</ol>
</div>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="step-2-ai-modelling-with-yolov4-and-cutie">
<h1>Step 2: AI modelling with YOLOv4 and CUTIE<a class="headerlink" href="#step-2-ai-modelling-with-yolov4-and-cutie" title="Permalink to this headline">¶</a></h1>
<div class="section" id="yolov4">
<h2>2.1 YOLOv4<a class="headerlink" href="#yolov4" title="Permalink to this headline">¶</a></h2>
<p>YOLOv4 (You Only Look Once version 4) is a one-stage object detection model that improves on YOLOv3 with several bags of
tricks and modules introduced in the literature.</p>
<p>The original YOLO is a clever Convolution Neural Network (CNN) for doing object detection in real-time. Its algorithm
applies a single neural network to the full image, divides the image into regions and predicts bounding boxes and
probabilities for each region.</p>
<div class="section" id="data-preprocessing">
<h3>2.1.1 Data preprocessing<a class="headerlink" href="#data-preprocessing" title="Permalink to this headline">¶</a></h3>
<p>Annotation files obtained after step 1 need to be reformatted to the YOLO input format. | Annotation files format | YOLO
input format |
|———————————————-|——————————————————————————-|
| .csv files | .txt files | | Several images / file | 1 image / file | | Polygonal bounding boxes | Rectangle bounding
boxes | | Absolute coordinates of bounding box corners | Relative coordinates of bounding boxes center, width and height
| | Boxes can be partially outside the picture | Boxes must be completely inside the picture |</p>
<p>The final format is a .txt file containing one line by bounding box with the following format :
{box-class} {x} {y} {box-width} {box-height} Each picture with its associated .txt annotation file must have the same
base name and be grouped in a unique folder. A helper function has been implemented to download the pictures and write
the associated txt file for each annotation files from step 1 : tools.yolo.preprocessing.convert_via_to_yolo</p>
</div>
<div class="section" id="installation">
<h3>2.1.2 Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h3>
<p>If you don’t want to use docker you can pass this steps and compile Darket directly (more
information <a class="reference external" href="https://github.com/AlexeyAB/darknet">here</a>)</p>
<div class="section" id="install-nvidia-docker">
<h4>2.1.2.1 Install Nvidia docker<a class="headerlink" href="#install-nvidia-docker" title="Permalink to this headline">¶</a></h4>
<ol class="simple">
<li><p>Install <a class="reference external" href="https://docs.docker.com/engine/install/">docker</a>
and <a class="reference external" href="https://docs.docker.com/compose/install/">docker compose</a></p></li>
<li><p>Install Nvidia driver (use your package manager and distribution
documentation (<a class="reference external" href="https://wiki.debian.org/fr/NvidiaGraphicsDrivers">debian</a>
, <a class="reference external" href="https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/index.html">other</a>…))</p></li>
<li><p>Install
<a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#install-guide">nvidia docker</a>**</p></li>
</ol>
<p>** Nvidia docker is only available on: Amazon Linux, Open Suse, Debian, Centos, RHEL and Ubuntu.</p>
</div>
<div class="section" id="build-the-docker-image">
<h4>2.1.2.2 Build the docker image<a class="headerlink" href="#build-the-docker-image" title="Permalink to this headline">¶</a></h4>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker-compose build
</pre></div>
</div>
</div>
</div>
<div class="section" id="model-training">
<h3>2.1.3 Model training<a class="headerlink" href="#model-training" title="Permalink to this headline">¶</a></h3>
<ol class="simple">
<li><p>put the image with annotation file in data/obj folder</p></li>
<li><p>create train, validation and testing set</p></li>
</ol>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3 process.py
</pre></div>
</div>
<ol class="simple">
<li><p>add pretrained weight in the data folder</p></li>
</ol>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> yolo/docker/data/
wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights
</pre></div>
</div>
<ol class="simple">
<li><p>Start the container</p></li>
</ol>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker-compose up -d
</pre></div>
</div>
<ol class="simple">
<li><p>Enter in bash (replace container_name by the name of the container)</p></li>
</ol>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker <span class="nb">exec</span> -it container_name bash
</pre></div>
</div>
<ol class="simple">
<li><p>launch training</p></li>
</ol>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> /home/data
darknet detector train /home/data/obj.data /home/data/yolov4-custom.cfg /home/data/yolov4.conv.137 -dont_show -map
</pre></div>
</div>
</div>
<div class="section" id="model-evaluation">
<h3>2.1.4 Model evaluation<a class="headerlink" href="#model-evaluation" title="Permalink to this headline">¶</a></h3>
<ol class="simple">
<li><p>Put the testing set path in the obj.data for valid and run</p></li>
</ol>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>darknet detector map /home/data/obj.data /home/data/yolov4-custom.cfg /home/data/trainning/yolov4-custom_best.weights
</pre></div>
</div>
<p><img alt="via0" src="_images/test.png" /></p>
<p>Mean average precision (mAP) measures average precision for recall value across all classes; mAP calculated at a default
IoU of 0.5 = 67.82%</p>
<p>F1 score measures balance between precision and recall; F1 score = 0.84</p>
<p>NOTE: The Opteeq dataset of approximately 1,300 data points may be too small to provide enough data for the model</p>
</div>
<div class="section" id="detection">
<h3>2.1.4 Detection<a class="headerlink" href="#detection" title="Permalink to this headline">¶</a></h3>
<p>If you want to use detection without docker replace libdarknet.so (in the yolo file of the repository opteeq) by your
libdarknet.so obtained after compilation. (you can’t use the libdarknet.so of the repository)</p>
<p>To get the image with a bounding box and the text extract</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3 -m yolo.predict --img image_path
</pre></div>
</div>
<p><img alt="darknet" src="_images/darknet.png" />
<img alt="text" src="_images/darknet_text.png" /></p>
</div>
<div class="section" id="detection-without-darknet-framework">
<h3>2.1.4 Detection without Darknet framework<a class="headerlink" href="#detection-without-darknet-framework" title="Permalink to this headline">¶</a></h3>
<p>Deploy this model in web application is complicated because it depends on darknet framework. In order to have an
serverless architecture it is better to avoid the dependencies to darknet.</p>
<p>In this part I use only OpenCv to make the prediction (more information to use darknet directly with
opencv <a class="reference external" href="https://opencv-tutorial.readthedocs.io/en/latest/yolo/yolo.html">here</a>).</p>
<ol class="simple">
<li><p>get the weights after training</p></li>
<li><p>edit path in <code class="docutils literal notranslate"><span class="pre">predict_opencv.py</span></code> for yolo config file</p></li>
<li></li>
</ol>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3 -m yolo.predict_opencv --img image_path
</pre></div>
</div>
<p><img alt="opencv" src="_images/opencv.png" />
<img alt="text" src="_images/darknet_text.png" /></p>
</div>
</div>
<div class="section" id="cutie">
<h2>2.2 CUTIE<a class="headerlink" href="#cutie" title="Permalink to this headline">¶</a></h2>
<p>CUTIE stands for Convolutional Universal Text Information Extractor. It is a model proposed by Xiaohui Zhao, Endi Niu,
Zhuo Wu, and Xiaoguang Wang which specific purpose is to extract information from text documents like receipts. The
academic paper (https://arxiv.org/pdf/1903.12363.pdf) has been implemented in Tensorflow by the authors. We propose here
a pytorch implementation of CUTIE. The advantage of this model, compared to YOLO, is that it uses the semantic
information of the receipts on top of the spatial positionning of the words.</p>
<div class="section" id="id1">
<h3>2.2.1 Data preprocessing<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>The annotation files from step 1 are first pre-processed to obtain a grid of words with their associated classes. The
resulting grid of each picture is stored in a JSON files.
<img alt="image" src="_images/grid.png" /></p>
<p>The json files are then used as inputs for a pytorch dataset module. This module takes care of :</p>
<ul class="simple">
<li><p>The word embedding, using a DistilBERT pretrained embedding</p></li>
<li><p>The formatting of the data into an input and a target tensor
<img alt="image" src="_images/pre-processing.png" /></p></li>
</ul>
</div>
<div class="section" id="id2">
<h3>2.1.2 Model training<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<ol class="simple">
<li><p>put the .JSON annotation files in cutie/data/train and cutie/data/val folders. Example files can be found in
cutie/data/example.</p></li>
<li><p>Build the docker image</p></li>
</ol>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> cutie
docker-compose build
</pre></div>
</div>
<ol class="simple">
<li><p>Start the container</p></li>
</ol>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker-compose up -d
</pre></div>
</div>
<ol class="simple">
<li><p>Enter in bash (replace container_name by the name of the container)</p></li>
</ol>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker <span class="nb">exec</span> -it container_name bash
</pre></div>
</div>
<ol class="simple">
<li><p>launch training (replace N_epochs, B_size, G_size by the number of epochs, the batch size and the grid size wanted
for the training)</p></li>
</ol>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python -m cutie.train --epochs N_epochs --batch_size B_size --grid_size G_size
</pre></div>
</div>
<p>After the training, the metrics file and the model checkpoints can be found in cutie/outputs</p>
</div>
</div>
<div class="section" id="models-benchmarking">
<h2>2.3 Models Benchmarking<a class="headerlink" href="#models-benchmarking" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="conclusion">
<h2>2.4 Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">¶</a></h2>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="step-3-web-application-deployment">
<h1>Step 3: Web application deployment<a class="headerlink" href="#step-3-web-application-deployment" title="Permalink to this headline">¶</a></h1>
<p>We build a serverless API with aws lambda functions and a static web app to consume this API.</p>
<p>In this app it is possible to choose between darknet or opencv in order to see the difference in terms of result and
time.</p>
<div class="section" id="deployement">
<h2>3.1 Deployement<a class="headerlink" href="#deployement" title="Permalink to this headline">¶</a></h2>
<div class="section" id="lambda">
<h3>3.1.1 lambda<a class="headerlink" href="#lambda" title="Permalink to this headline">¶</a></h3>
<ol class="simple">
<li><p>build the container</p></li>
</ol>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker build -t opteeq .    
</pre></div>
</div>
<p>Use the dockerfile at the racine of the project, don’t use the docker file with cuda support this container is too big.</p>
<p>This container has runtime interface emulator you can test your lambda function localy with.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>docker run -p <span class="m">9000</span>:8080 opteeq
curl -XPOST <span class="s2">&quot;http://localhost:9000/2015-03-31/functions/function/invocations&quot;</span> -d <span class="s1">&#39;{}&#39;</span>
</pre></div>
</div>
<ol class="simple">
<li><p>push the container to AWS ECR (more info <a class="reference external" href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html">here</a>)</p></li>
<li><p>create a lambda function which use this image</p></li>
<li><p>create an API gateway</p></li>
</ol>
</div>
<div class="section" id="static-file">
<h3>3.1.2 Static file<a class="headerlink" href="#static-file" title="Permalink to this headline">¶</a></h3>
<p>We choose to distribute the static file with AWS s3 for simplicity and cost.</p>
<ol class="simple">
<li><p>upload the two file of serverless/static folder in a bucket</p></li>
<li><p>follow <a class="reference external" href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/HostingWebsiteOnS3Setup.html">this</a> tutorial to serve
static files</p></li>
</ol>
<p>credit: app.html inspiration <a class="reference external" href="https://github.com/amtam0/lambda-tesseract-api/blob/master/webapp/app.html">link</a></p>
</div>
<div class="section" id="demo">
<h3>3.1.3 Demo<a class="headerlink" href="#demo" title="Permalink to this headline">¶</a></h3>
<p>You can access to a demo <a class="reference external" href="http://api-opteeq.s3-website.eu-west-3.amazonaws.com/">here</a>.</p>
</div>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="license">
<h1>License<a class="headerlink" href="#license" title="Permalink to this headline">¶</a></h1>
<p>:::info TBD
:::</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="team">
<h1>Team<a class="headerlink" href="#team" title="Permalink to this headline">¶</a></h1>
<p>This project was developed for the DSTI S21 Python Labs class by S. Adimabua Anonyai, P. Bayona, L. Bonnand Germain, J.
Griffiths, W. Hu, S. Koffi Fanoukoe &amp; J. Yates, under the supervision of A. Sanogo.</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="sphinx-documentation">
<h1>Sphinx Documentation<a class="headerlink" href="#sphinx-documentation" title="Permalink to this headline">¶</a></h1>
<p>The documentation is in docs folder. It is also hosted <a class="reference external" href="https://assansanogo.github.io/Opteeq/build/html/info.html">here</a>
.</p>
<p>To generate documentation:</p>
<ol class="simple">
<li><p>Edit the README.md or the different docStrings. You can also add a reStructuredText file (rst) or markdown (md) file
in source.</p></li>
<li><p>Install required python library <code class="docutils literal notranslate"><span class="pre">pip3</span> <span class="pre">install</span> <span class="pre">-r</span> <span class="pre">requirements.txt</span></code>. You need sphinx, sphinx_rtd_theme and
myst_parser.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sphinx-build</span> <span class="pre">-b</span> <span class="pre">html</span> <span class="pre">docs/source</span> <span class="pre">docs/build/html</span> </code></p></li>
</ol>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="modules.html" class="btn btn-neutral float-right" title="opteeq" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="index.html" class="btn btn-neutral float-left" title="&lt;no title&gt;" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Léo.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>